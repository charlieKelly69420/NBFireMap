name: Fetch ERD Fires JSON

on:
  workflow_dispatch:
  schedule:
    - cron: "38 * * * *"

permissions:
  contents: write

concurrency:
  group: fetch-erd-fires
  cancel-in-progress: false

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest

    env:
      ACTIVE_URL: "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=json"
      OUT_URL:    "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/1/query?where=1%3D1&outFields=*&returnGeometry=true&f=json"
      SUMS_URL:   "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/2/query?where=1%3D1&outFields=*&returnGeometry=true&f=json"

      COPIES_PER_PASS: "10"
      SLEEP_BETWEEN_COPIES_SEC: "8"
      CHUNK_SIZE: "1000"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Fetch ERD layers (one pass, save 10 copies + pick best)
        id: fetch
        shell: python
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          import os, json, time, hashlib, datetime, requests, copy
          from pathlib import Path

          # ---------- config ----------
          targets = {
              "active_fires.json": os.environ["ACTIVE_URL"],
              "out_fires.json":    os.environ["OUT_URL"],
              "sums_table.json":   os.environ["SUMS_URL"],
          }

          COPIES       = int(os.environ.get("COPIES_PER_PASS", "10"))
          SLEEP_COPIES = int(os.environ.get("SLEEP_BETWEEN_COPIES_SEC", "8"))
          CHUNK_SIZE   = int(os.environ.get("CHUNK_SIZE", "1000"))

          session = requests.Session()
          session.headers.update({
              "User-Agent": "github-actions-fetch-erd/3.0",
              "Cache-Control": "no-cache",
              "Pragma": "no-cache",
          })

          def _layer_base(url):
              # Normalize to deterministic base: <layer>/query?where=1=1&outFields=*
              if "/query?" in url:
                  base = url.split("/query?")[0]
              else:
                  base = url.rstrip("/")
              return base + "/query?where=1%3D1&outFields=*"

          def _ts():
              return str(int(time.time() * 1000))

          def req(url, timeout=60):
              r = session.get(url + ("&_ts=" + _ts()), timeout=timeout)
              r.raise_for_status()
              return r.json()

          def get_all_ids(base):
              j = req(base + "&returnIdsOnly=true&f=json")
              ids = j.get("objectIds") or []
              ids = [int(x) for x in ids]
              ids.sort()
              # count (informational)
              cnt = req(base + "&returnCountOnly=true&f=json").get("count", 0)
              return ids, int(cnt)

          def fetch_chunk(base, id_chunk):
              ids_str = ",".join(str(i) for i in id_chunk)
              url = (base
                     + f"&objectIds={ids_str}"
                     + "&orderByFields=OBJECTID"
                     + "&returnGeometry=true"
                     + "&f=json")
              return req(url)

          def fetch_full_ids_first(url):
              """
              Deterministically fetch ALL features for a layer/table using IDs-first.
              Returns ESRI JSON + meta with counts.
              """
              base = _layer_base(url)
              ids, expected_count = get_all_ids(base)
              features = []
              fields = None
              geometryType = None
              spatialReference = None
              objectIdFieldName = None

              for start in range(0, len(ids), CHUNK_SIZE):
                  chunk = ids[start:start+CHUNK_SIZE]
                  j = fetch_chunk(base, chunk)
                  feats = j.get("features") or []
                  features.extend(feats)
                  if fields is None: fields = j.get("fields")
                  if geometryType is None: geometryType = j.get("geometryType")
                  if spatialReference is None: spatialReference = j.get("spatialReference")
                  if objectIdFieldName is None: objectIdFieldName = j.get("objectIdFieldName")

              esri = {"features": features}
              if fields is not None: esri["fields"] = fields
              if geometryType is not None: esri["geometryType"] = geometryType
              if spatialReference is not None: esri["spatialReference"] = spatialReference
              if objectIdFieldName is not None: esri["objectIdFieldName"] = objectIdFieldName

              meta = {
                  "ids_len": len(ids),
                  "expected_count": expected_count,
                  "fetched_len": len(features),
                  "ok_len": (len(ids) == expected_count == len(features)),
              }
              return esri, meta

          def adorn_fetch_time(esri_json, timestamp_iso):
              esri_json = copy.deepcopy(esri_json)
              fields = esri_json.get("fields", [])
              if not isinstance(fields, list):
                  fields = []
              if not any((f or {}).get("name") == "ERD_FETCH_TIME" for f in fields):
                  fields.append({
                      "name": "ERD_FETCH_TIME",
                      "type": "esriFieldTypeString",
                      "alias": "ERD_FETCH_TIME",
                      "sqlType": "sqlTypeOther",
                      "length": 32,
                      "nullable": True,
                      "editable": False,
                      "domain": None,
                      "defaultValue": None
                  })
              esri_json["fields"] = fields
              for feat in esri_json.get("features", []) or []:
                  attrs = feat.setdefault("attributes", {})
                  attrs["ERD_FETCH_TIME"] = timestamp_iso
              return esri_json

          def extract_attrs_digest(esri_json):
              feats = esri_json.get("features", []) or []
              attrs_list_sorted = []
              for f in feats:
                  attrs = (f or {}).get("attributes", {}) or {}
                  attrs_list_sorted.append(json.dumps(attrs, sort_keys=True))
              attrs_list_sorted.sort()
              digest_src = "[" + ",".join(attrs_list_sorted) + "]"
              return hashlib.sha256(digest_src.encode("utf-8")).hexdigest()

          # One pass over all targets, always complete, save all copies + best
          summary_lines = []
          for fname, url in targets.items():
              print(f"::group::Processing {fname}")
              copies = []
              metas = []
              digests = []
              ts_iso = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

              # Fetch N copies
              for i in range(COPIES):
                  esri, meta = fetch_full_ids_first(url)
                  esri = adorn_fetch_time(esri, ts_iso)
                  copies.append(esri)
                  metas.append(meta)
                  digest = extract_attrs_digest(esri)
                  digests.append(digest)
                  print(f"  Copy {i+1}/{COPIES}: fetched={meta['fetched_len']}, expected={meta['expected_count']}, ids={meta['ids_len']}, ok={meta['ok_len']}, attrs-digest={digest[:12]}…", flush=True)
                  # Save each copy
                  stem = Path(fname).stem
                  copy_name = f"{stem}.copy{(i+1):02d}.json"
                  with open(copy_name, "w", encoding="utf-8") as f:
                      json.dump(esri, f, ensure_ascii=False, indent=2)
                  if i < COPIES - 1:
                      time.sleep(SLEEP_COPIES)

              # Choose best: the one with the most features; tie → latest
              best_idx = max(range(len(copies)), key=lambda k: (metas[k]["fetched_len"], k))
              best = copies[best_idx]
              with open(fname, "w", encoding="utf-8") as f:
                  json.dump(best, f, ensure_ascii=False, indent=2)

              # Write a small meta report per target
              report = {
                  "target": fname,
                  "chosen_copy_index": best_idx + 1,
                  "chosen_fetched_len": metas[best_idx]["fetched_len"],
                  "copies": [
                      {
                          "index": i+1,
                          "fetched_len": m["fetched_len"],
                          "expected_count": m["expected_count"],
                          "ids_len": m["ids_len"],
                          "ok_len": m["ok_len"],
                          "attrs_digest": digests[i],
                      } for i, m in enumerate(metas)
                  ]
              }
              meta_name = f"{Path(fname).stem}.meta.json"
              with open(meta_name, "w", encoding="utf-8") as f:
                  json.dump(report, f, ensure_ascii=False, indent=2)

              # Summary line
              same = all(d == digests[0] for d in digests[1:])
              summary_lines.append(
                  f"- {fname}: chosen copy #{best_idx+1} (features={metas[best_idx]['fetched_len']}); "
                  f"all-10-match={same}"
              )
              print(f"→ Wrote {fname} from copy #{best_idx+1}; all-10-match={same}", flush=True)
              print("::endgroup::")

          # Job summary (no failure/exit)
          summary_path = os.environ.get("GITHUB_STEP_SUMMARY")
          if summary_path:
              Path(summary_path).write_text("### ERD Fetch (one pass, saved 10 copies + best)\n" + "\n".join(summary_lines) + "\n")

      - name: Commit updated JSONs
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): one-pass ERD fetch — save 10 copies + best (IDs-first) + meta"
          file_pattern: |
            active_fires.json
            out_fires.json
            sums_table.json
            active_fires.copy*.json
            out_fires.copy*.json
            sums_table.copy*.json
            active_fires.meta.json
            out_fires.meta.json
            sums_table.meta.json
