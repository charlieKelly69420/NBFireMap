name: Fetch ERD Fires JSON

on:
  workflow_dispatch:
  schedule:
    - cron: "38 * * * *"

permissions:
  contents: write

concurrency:
  group: fetch-erd-fires
  cancel-in-progress: false

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest

    env:
      ACTIVE_URL: "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=json"
      OUT_URL:    "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/1/query?where=1%3D1&outFields=*&returnGeometry=true&f=json"
      SUMS_URL:   "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/2/query?where=1%3D1&outFields=*&returnGeometry=true&f=json"

      COPIES_PER_PASS: "10"
      SLEEP_BETWEEN_COPIES_SEC: "8"
      SLEEP_BETWEEN_PASSES_SEC: "60"
      MAX_PASSES: "5"
      CHUNK_SIZE: "1000"

      # Geometry closeness tolerance (meters). If the layer is in EPSG:4326, we'll great-circle it.
      # Otherwise we assume layer units are meters and use Euclidean distance.
      TOL_METERS: "10"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Fetch ERD layers (IDs-first; compare text attrs + coordinate closeness; retry failed)
        id: fetch
        shell: python
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          import os, json, time, sys, datetime, requests, copy, math
          from pathlib import Path

          # ---------- config ----------
          targets = {
              "active_fires.json": os.environ["ACTIVE_URL"],
              "out_fires.json":    os.environ["OUT_URL"],
              "sums_table.json":   os.environ["SUMS_URL"],
          }

          COPIES       = int(os.environ.get("COPIES_PER_PASS", "10"))
          SLEEP_COPIES = int(os.environ.get("SLEEP_BETWEEN_COPIES_SEC", "8"))
          SLEEP_PASSES = int(os.environ.get("SLEEP_BETWEEN_PASSES_SEC", "60"))
          MAX_PASSES   = int(os.environ.get("MAX_PASSES", "5"))
          CHUNK_SIZE   = int(os.environ.get("CHUNK_SIZE", "1000"))
          TOL_METERS   = float(os.environ.get("TOL_METERS", "10"))

          session = requests.Session()
          session.headers.update({
              "User-Agent": "github-actions-fetch-erd/4.0",
              "Cache-Control": "no-cache",
              "Pragma": "no-cache",
          })

          # ---------- helpers ----------
          def _layer_base(url):
              # Normalize to deterministic base: <layer>/query?where=1=1&outFields=*
              if "/query?" in url:
                  base = url.split("/query?")[0]
              else:
                  base = url.rstrip("/")
              return base + "/query?where=1%3D1&outFields=*"

          def _ts():
              return str(int(time.time() * 1000))

          def req(url, timeout=60):
              r = session.get(url + ("&_ts=" + _ts()), timeout=timeout)
              r.raise_for_status()
              return r.json()

          def get_all_ids(base):
              j = req(base + "&returnIdsOnly=true&f=json")
              ids = j.get("objectIds") or []
              ids = [int(x) for x in ids]
              ids.sort()
              cnt = req(base + "&returnCountOnly=true&f=json").get("count", 0)
              return ids, int(cnt)

          def fetch_chunk(base, id_chunk):
              ids_str = ",".join(str(i) for i in id_chunk)
              url = (base
                     + f"&objectIds={ids_str}"
                     + "&orderByFields=OBJECTID"
                     + "&returnGeometry=true"
                     + "&f=json")
              return req(url)

          def fetch_full_ids_first(url):
              """
              Deterministically fetch ALL features for a layer/table using IDs-first.
              Returns ESRI JSON + meta with counts and schema bits.
              """
              base = _layer_base(url)
              ids, expected_count = get_all_ids(base)
              features = []
              fields = None
              geometryType = None
              spatialReference = None
              objectIdFieldName = None

              for start in range(0, len(ids), CHUNK_SIZE):
                  chunk = ids[start:start+CHUNK_SIZE]
                  j = fetch_chunk(base, chunk)
                  feats = j.get("features") or []
                  features.extend(feats)
                  if fields is None: fields = j.get("fields")
                  if geometryType is None: geometryType = j.get("geometryType")
                  if spatialReference is None: spatialReference = j.get("spatialReference")
                  if objectIdFieldName is None: objectIdFieldName = j.get("objectIdFieldName")

              esri = {"features": features}
              if fields is not None: esri["fields"] = fields
              if geometryType is not None: esri["geometryType"] = geometryType
              if spatialReference is not None: esri["spatialReference"] = spatialReference
              if objectIdFieldName is not None: esri["objectIdFieldName"] = objectIdFieldName

              meta = {
                  "ids_len": len(ids),
                  "expected_count": expected_count,
                  "fetched_len": len(features),
                  "ok_len": (len(ids) == expected_count == len(features)),
              }
              return esri, meta

          def adorn_fetch_time(esri_json, timestamp_iso):
              esri_json = copy.deepcopy(esri_json)
              fields = esri_json.get("fields", [])
              if not isinstance(fields, list):
                  fields = []
              if not any((f or {}).get("name") == "ERD_FETCH_TIME" for f in fields):
                  fields.append({
                      "name": "ERD_FETCH_TIME",
                      "type": "esriFieldTypeString",
                      "alias": "ERD_FETCH_TIME",
                      "sqlType": "sqlTypeOther",
                      "length": 32,
                      "nullable": True,
                      "editable": False,
                      "domain": None,
                      "defaultValue": None
                  })
              esri_json["fields"] = fields
              for feat in esri_json.get("features", []) or []:
                  attrs = feat.setdefault("attributes", {})
                  attrs["ERD_FETCH_TIME"] = timestamp_iso
              return esri_json

          # ---- comparison helpers (text attrs + point coordinate closeness) ----
          def string_field_names(fields):
              names = []
              for f in (fields or []):
                  try:
                      if (f or {}).get("type") == "esriFieldTypeString":
                          names.append((f.get("name") or "").strip())
                  except Exception:
                      pass
              # always exclude the synthetic field
              return [n for n in names if n and n != "ERD_FETCH_TIME"]

          def objectid_field_name(esri_json):
              name = esri_json.get("objectIdFieldName")
              if name:
                  return name
              # fallback
              return "OBJECTID"

          def features_by_oid(esri_json, oid_field):
              out = {}
              for feat in esri_json.get("features", []) or []:
                  attrs = (feat or {}).get("attributes", {}) or {}
                  oid = attrs.get(oid_field)
                  if oid is not None:
                      out[int(oid)] = feat
              return out

          def haversine_meters(lon1, lat1, lon2, lat2):
              R = 6371000.0
              to_rad = math.radians
              dlat = to_rad(lat2 - lat1)
              dlon = to_rad(lon2 - lon1)
              a = math.sin(dlat/2)**2 + math.cos(to_rad(lat1))*math.cos(to_rad(lat2))*math.sin(dlon/2)**2
              c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
              return R * c

          def point_distance_meters(geom1, geom2, spatial_ref):
              # supports only esriGeometryPoint comparison
              if not geom1 or not geom2:
                  return float("inf")
              x1, y1 = geom1.get("x"), geom1.get("y")
              x2, y2 = geom2.get("x"), geom2.get("y")
              if x1 is None or y1 is None or x2 is None or y2 is None:
                  return float("inf")

              wkid = None
              if spatial_ref and isinstance(spatial_ref, dict):
                  wkid = spatial_ref.get("latestWkid") or spatial_ref.get("wkid")

              if wkid == 4326:
                  # x=lon, y=lat in degrees
                  return haversine_meters(x1, y1, x2, y2)
              else:
                  # assume meters
                  dx = float(x1) - float(x2)
                  dy = float(y1) - float(y2)
                  return math.hypot(dx, dy)

          def copies_match_text_and_coords(copies):
              """
              Returns (match: bool, report: dict)
              - match requires:
                * same set of OBJECTIDs across copies
                * for each OID: all string attributes equal
                * if geometryType == esriGeometryPoint: point distance <= TOL_METERS across copies
              """
              if not copies:
                  return False, {"reason": "no copies"}

              base = copies[0]
              gtype = base.get("geometryType")
              sref = base.get("spatialReference")
              fields = base.get("fields") or []
              sfields = string_field_names(fields)
              oidf = objectid_field_name(base)

              # build OID maps for each copy
              maps = [features_by_oid(c, oidf) for c in copies]

              # same OID sets?
              base_oids = set(maps[0].keys())
              for i, m in enumerate(maps[1:], start=2):
                  if set(m.keys()) != base_oids:
                      return False, {"reason": f"OID set mismatch at copy {i}"}

              # per-oid checks
              for oid in sorted(base_oids):
                  # string attrs equal?
                  base_attrs = (maps[0][oid].get("attributes") or {})
                  for i, m in enumerate(maps[1:], start=2):
                      attrs = (m[oid].get("attributes") or {})
                      for fn in sfields:
                          if str(base_attrs.get(fn)) != str(attrs.get(fn)):
                              return False, {"reason": f"text attr mismatch", "oid": oid, "field": fn, "copy": i}

                  # coordinates close? (only for point layers)
                  if gtype == "esriGeometryPoint":
                      base_geom = maps[0][oid].get("geometry")
                      for i, m in enumerate(maps[1:], start=2):
                          geom = m[oid].get("geometry")
                          dist = point_distance_meters(base_geom, geom, sref)
                          if not (dist == dist):  # NaN guard
                              return False, {"reason": "distance NaN", "oid": oid, "copy": i}
                          if dist > TOL_METERS:
                              return False, {"reason": "geom too far", "oid": oid, "copy": i, "distance_m": dist}

              return True, {"reason": "all good"}

          # ---------- main driver (passes with retries on fail) ----------
          remaining = dict(targets)   # fname -> url
          results   = {fn: None for fn in targets}

          print(f"\n=== Starting fetch (IDs-first; verify text attrs + coordinate closeness) ===", flush=True)
          for attempt in range(1, MAX_PASSES + 1):
              if not remaining:
                  break

              print(f"\n== Pass {attempt}/{MAX_PASSES} – processing {len(remaining)} target(s) ==", flush=True)
              failed_this_pass = {}

              for fname, url in list(remaining.items()):
                  print(f"::group::Processing {fname}")

                  copies = []
                  metas  = []
                  # fetch N copies
                  for i in range(COPIES):
                      esri, meta = fetch_full_ids_first(url)
                      metas.append(meta)
                      copies.append(esri)
                      print(f"  Copy {i+1}/{COPIES}: fetched={meta['fetched_len']}, expected={meta['expected_count']}, ids={meta['ids_len']}, counts_ok={meta['ok_len']}", flush=True)
                      if i < COPIES - 1:
                          time.sleep(SLEEP_COPIES)

                  # basic count sanity: all copies must have counts_ok and equal fetched_len
                  counts_ok = all(m.get("ok_len") for m in metas)
                  if not counts_ok:
                      print("  ✖ Count mismatch across copies (or ID list) — will retry.", flush=True)
                      results[fname] = False
                      failed_this_pass[fname] = url
                      print("::endgroup::")
                      continue

                  # verify text attributes and coordinate closeness
                  match, report = copies_match_text_and_coords(copies)
                  if match:
                      print("  ✅ Text attributes match and coordinates are within tolerance across copies.", flush=True)
                      ts = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
                      final = adorn_fetch_time(copies[0], ts)
                      with open(fname, "w", encoding="utf-8") as f:
                          json.dump(final, f, ensure_ascii=False, indent=2)
                      print(f"  → Wrote {fname} with ERD_FETCH_TIME={ts}", flush=True)
                      results[fname] = True
                  else:
                      print(f"  ✖ Verification failed: {report}", flush=True)
                      results[fname] = False
                      failed_this_pass[fname] = url

                  print("::endgroup::")

              if failed_this_pass and attempt < MAX_PASSES:
                  print(f"\nSome targets failed verification; retrying them after {SLEEP_PASSES}s…", flush=True)
                  time.sleep(SLEEP_PASSES)

              remaining = failed_this_pass

          # ---------- summary ----------
          print("\n=== SUMMARY ===", flush=True)
          ok_all = True
          lines = []
          for fn in targets:
              status = results.get(fn)
              if status is True:
                  msg = f"{fn}: OK"
              elif status is False:
                  msg = f"{fn}: FAILED"
                  ok_all = False
              else:
                  msg = f"{fn}: SKIPPED?"
                  ok_all = False
              print(" " + msg, flush=True)
              lines.append(f"- {msg}")

          summary_path = os.environ.get("GITHUB_STEP_SUMMARY")
          if summary_path:
              Path(summary_path).write_text(
                  "### ERD Fetch (text-only attrs + coordinate closeness)\n"
                  f"- Tolerance: {TOL_METERS} m\n"
                  + "\n".join(lines) + "\n"
              )

          if not ok_all:
              sys.exit(1)

      - name: Commit updated JSONs
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): update ERD fires JSONs (text-attrs + coord closeness verified across copies)"
          file_pattern: |
            active_fires.json
            out_fires.json
            sums_table.json
