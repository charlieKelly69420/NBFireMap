name: Fetch ERD Fires GeoJSON

on:
  workflow_dispatch:
  schedule:
    - cron: "38 * * * *"

permissions:
  contents: write

concurrency:
  group: fetch-erd-fires
  cancel-in-progress: false

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest

    env:
      ACTIVE_URL: "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/0/query?where=1%3D1&outFields=*&returnGeometry=true"
      OUT_URL:    "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/1/query?where=1%3D1&outFields=*&returnGeometry=true"
      SUMS_URL:   "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/2/query?where=1%3D1&outFields=*&returnGeometry=true"

      COPIES_PER_PASS: "10"
      SLEEP_BETWEEN_COPIES_SEC: "8"
      SLEEP_BETWEEN_PASSES_SEC: "60"
      MAX_PASSES: "5"
      CHUNK_SIZE: "800"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests

      # Clean up any previously-committed files in wrong locations or with wrong names
      - name: Clean old/misplaced outputs
        shell: bash
        run: |
          set -euxo pipefail
          # Remove any duplicates not in repo root
          find . -type f -name 'active_fires.geojson' ! -path './active_fires.geojson' -delete || true
          find . -type f -name 'out_fires.geojson'    ! -path './out_fires.geojson'    -delete || true
          find . -type f -name 'sums_table.json'      ! -path './sums_table.json'      -delete || true
          # Remove old/mistyped filenames anywhere
          find . -type f -name 'sums_table.geojson' -delete || true
          find . -type f -name 'out_fires_geojson'  -delete || true

      - name: Fetch ERD layers (strict verify; write only on success)
        id: fetch
        shell: python
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          import os, json, time, sys, datetime, requests, copy
          from pathlib import Path

          # ---------- config ----------
          targets = {
              "active_fires.geojson": os.environ["ACTIVE_URL"],
              "out_fires.geojson":    os.environ["OUT_URL"],
              "sums_table.json":      os.environ["SUMS_URL"],   # table saved as JSON (attrs only)
          }

          COPIES       = int(os.environ.get("COPIES_PER_PASS", "10"))
          SLEEP_COPIES = int(os.environ.get("SLEEP_BETWEEN_COPIES_SEC", "8"))
          SLEEP_PASSES = int(os.environ.get("SLEEP_BETWEEN_PASSES_SEC", "60"))
          MAX_PASSES   = int(os.environ.get("MAX_PASSES", "5"))
          CHUNK_SIZE   = int(os.environ.get("CHUNK_SIZE", "800"))

          session = requests.Session()
          session.headers.update({
              "User-Agent": "github-actions-fetch-erd/geojson-strings-only/1.2",
              "Cache-Control": "no-cache",
              "Pragma": "no-cache",
          })

          # ---------- helpers ----------
          def _layer_root(url):
              return url.split("/query")[0].rstrip("?&")

          def _base_query(url):
              return _layer_root(url) + "/query?where=1%3D1&outFields=*"

          def _ts_param():
              return {"_ts": str(int(time.time() * 1000))}

          def get_json(method, url, params=None):
              params = (params or {}).copy()
              params.update(_ts_param())
              if method == "GET":
                  r = session.get(url, params=params, timeout=120)
              else:
                  r = session.post(url, data=params, timeout=120)
              r.raise_for_status()
              return r.json()

          def layer_info(url):
              info = get_json("GET", _layer_root(url), {"f":"json"})
              fields = info.get("fields") or []
              gtype  = info.get("geometryType")
              oidf   = info.get("objectIdField") or info.get("objectIdFieldName") or "OBJECTID"
              return fields, gtype, oidf

          def get_all_ids(base):
              j = get_json("GET", base, {"returnIdsOnly":"true","f":"json"})
              ids = j.get("objectIds") or []
              try: ids = [int(x) for x in ids]
              except Exception: ids = list(ids)
              ids.sort()
              cnt = get_json("GET", base, {"returnCountOnly":"true","f":"json"}).get("count", 0)
              return ids, int(cnt)

          def fetch_chunk_esri_json(base, id_chunk):
              params = {
                  "objectIds": ",".join(str(i) for i in id_chunk),
                  "orderByFields": "OBJECTID",
                  "returnGeometry": "true",
                  "outFields": "*",
                  "outSR": "4326",
                  "f": "json",
              }
              return get_json("POST", base, params)

          # ---- ESRI JSON -> GeoJSON (points supported; others -> null geom) ----
          def esri_feature_to_geojson(f, geometry_type):
              props = (f or {}).get("attributes") or {}
              geom  = (f or {}).get("geometry")
              gj_geom = None
              if geometry_type == "esriGeometryPoint" and geom:
                  x, y = geom.get("x"), geom.get("y")
                  if x is not None and y is not None:
                      gj_geom = {"type":"Point","coordinates":[float(x), float(y)]}
              return {"type":"Feature","geometry":gj_geom,"properties":props}

          def esri_to_geojson(esri_json, geometry_type):
              feats = esri_json.get("features") or []
              return {"type":"FeatureCollection","features":[esri_feature_to_geojson(f, geometry_type) for f in feats]}

          def fetch_full_geojson(url):
              fields, gtype, oidf = layer_info(url)
              base = _base_query(url)
              ids, expected_count = get_all_ids(base)
              all_gj_features = []
              for start in range(0, len(ids), CHUNK_SIZE):
                  chunk = ids[start:start+CHUNK_SIZE]
                  esri_chunk = fetch_chunk_esri_json(base, chunk)
                  gj_chunk   = esri_to_geojson(esri_chunk, gtype)
                  all_gj_features.extend(gj_chunk.get("features", []))
              fc = {"type":"FeatureCollection","features": all_gj_features}
              meta = {
                  "ids_len": len(ids),
                  "expected_count": expected_count,
                  "fetched_len": len(all_gj_features),
                  "ok_len": (len(ids) == expected_count == len(all_gj_features)),
              }
              schema = {"fields": fields, "geometryType": gtype, "oid_field": oidf}
              return fc, meta, schema

          def stamp_fetch_time_on_fc(fc, ts_iso):
              out = copy.deepcopy(fc)
              for feat in out.get("features", []):
                  props = feat.setdefault("properties", {})
                  props["ERD_FETCH_TIME"] = ts_iso
              return out

          def fc_to_rows_with_stamp(fc, ts_iso):
              rows = []
              for feat in (fc.get("features") or []):
                  props = dict((feat or {}).get("properties") or {})
                  props["ERD_FETCH_TIME"] = ts_iso
                  rows.append(props)
              return rows

          # ---- compare string attributes only ----
          def string_field_names(fields):
              names = []
              for f in (fields or []):
                  if (f or {}).get("type") == "esriFieldTypeString":
                      n = (f.get("name") or "").strip()
                      if n and n != "ERD_FETCH_TIME":
                          names.append(n)
              return names

          def features_by_oid_geojson(fc, oid_field):
              m = {}
              for feat in fc.get("features", []) or []:
                  props = (feat or {}).get("properties") or {}
                  oid = props.get(oid_field)
                  if oid is not None:
                      try: m[int(oid)] = feat
                      except Exception: m[str(oid)] = feat
              return m

          def copies_match_string_props_only(copies, schema):
              if not copies: return False, {"reason": "no copies"}
              fields = schema.get("fields") or []
              oidf   = schema.get("oid_field") or "OBJECTID"
              sfields = string_field_names(fields)
              maps = [features_by_oid_geojson(c, oidf) for c in copies]
              base_oids = set(maps[0].keys())
              for i, m in enumerate(maps[1:], start=2):
                  if set(m.keys()) != base_oids:
                      return False, {"reason": f"OID set mismatch at copy {i}"}
              for oid in sorted(base_oids):
                  base_props = (maps[0][oid].get("properties") or {})
                  for i, m in enumerate(maps[1:], start=2):
                      props = (m[oid].get("properties") or {})
                      for fn in sfields:
                          if str(base_props.get(fn)) != str(props.get(fn)):
                              return False, {"reason": "string attr mismatch", "oid": oid, "field": fn, "copy": i}
              return True, {"reason": "all good"}

          # ---------- main driver ----------
          remaining = dict(targets)
          results   = {fn: None for fn in targets}

          print(f"\n=== Starting fetch (GeoJSON; string-attrs only; POST + local conversion) ===", flush=True)
          for attempt in range(1, MAX_PASSES + 1):
              if not remaining:
                  break

              print(f"\n== Pass {attempt}/{MAX_PASSES} â€“ processing {len(remaining)} target(s) ==", flush=True)
              failed_this_pass = {}

              for fname, url in list(remaining.items()):
                  print(f"::group::Processing {fname}")

                  copies = []
                  metas  = []

                  for i in range(COPIES):
                      fc, meta, schema = fetch_full_geojson(url)
                      copies.append(fc)
                      metas.append(meta)
                      print(f"  Copy {i+1}/{COPIES}: fetched={meta['fetched_len']}, expected={meta['expected_count']}, ids={meta['ids_len']}, counts_ok={meta['ok_len']}", flush=True)
                      if i < COPIES - 1:
                          time.sleep(SLEEP_COPIES)

                  counts_ok = all(m.get("ok_len") for m in metas)
                  if not counts_ok:
                      print("  âœ– Count mismatch across copies â€” will retry.", flush=True)
                      results[fname] = False
                      failed_this_pass[fname] = url
                      print("::endgroup::")
                      continue

                  match, report = copies_match_string_props_only(copies, schema)
                  if match:
                      print("  âœ… String attributes match across copies.", flush=True)
                      ts = datetime.datetime.now(datetime.UTC).replace(microsecond=0).isoformat().replace("+00:00","Z")

                      if fname == "sums_table.json":
                          # Write plain JSON (attribute rows) for the table
                          rows = fc_to_rows_with_stamp(copies[0], ts)
                          with open(fname, "w", encoding="utf-8") as f:
                              json.dump({"rows": rows}, f, ensure_ascii=False, indent=2)
                          print(f"  â†’ Wrote {fname} (rows JSON) with ERD_FETCH_TIME={ts}", flush=True)
                      else:
                          final = stamp_fetch_time_on_fc(copies[0], ts)
                          with open(fname, "w", encoding="utf-8") as f:
                              json.dump(final, f, ensure_ascii=False, indent=2)
                          print(f"  â†’ Wrote {fname} with ERD_FETCH_TIME={ts}", flush=True)

                      results[fname] = True
                  else:
                      print(f"  âœ– Verification failed: {report}", flush=True)
                      results[fname] = False
                      failed_this_pass[fname] = url

                  print("::endgroup::")

              if failed_this_pass and attempt < MAX_PASSES:
                  print(f"\nSome targets failed verification; retrying them after {SLEEP_PASSES}sâ€¦", flush=True)
                  time.sleep(SLEEP_PASSES)

              remaining = failed_this_pass

          # ---------- summary ----------
          print("\n=== SUMMARY ===", flush=True)
          ok_all = True
          lines = []
          for fn in targets:
              status = results.get(fn)
              if status is True:
                  msg = f"{fn}: OK"
              elif status is False:
                  msg = f"{fn}: FAILED"
                  ok_all = False
              else:
                  msg = f"{fn}: SKIPPED?"
                  ok_all = False
              print(" " + msg, flush=True)
              lines.append(f"- {msg}")

          summary_path = os.environ.get("GITHUB_STEP_SUMMARY")
          if summary_path:
              Path(summary_path).write_text(
                  "### ERD Fetch (GeoJSON; string-attrs only; POST + local conversion)\n"
                  + "\n".join(lines) + "\n"
              )

          if not ok_all:
              sys.exit(1)

      - name: Commit updated data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): update ERD fires (active/out geojson; sums json)"
          file_pattern: |
            active_fires.geojson
            out_fires.geojson
            sums_table.json
          # Stage deletions from the cleanup step, too
          commit_options: '--all'
