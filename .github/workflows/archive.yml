name: Fetch ERD Fire Datasets

on:
  schedule:
    # Runs at 21 minutes after every hour (UTC)
    - cron: "21 * * * *"
  workflow_dispatch: {}

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest
    env:
      PYTHONUNBUFFERED: "1"

      # --- ERD (ArcGIS) ---
      ACTIVE_FIRES_URL: "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/0/query"
      OUT_FIRES_URL: "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/1/query"
      SUMS_TABLE_URL: "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/2/query"
      ACTIVE_FIRES_FILE: "active_fires.geojson"
      OUT_FIRES_FILE: "out_fires.geojson"
      SUMS_TABLE_FILE: "sums_table.json"

      # --- CWFIS (Geoserver WFS) ---
      CWFIS_WFS_ENDPOINT: "https://cwfis.cfs.nrcan.gc.ca/geoserver/public/ows"
      CWFIS_HOTSPOTS_TYPENAME_24H: "public:hotspots_last24hrs"
      CWFIS_HOTSPOTS_24_FILE: "hotspots_last24hrs.geojson"

      # --- Clipping options ---
      DO_CLIP: "true"                       # set to "false" to bypass clip
      CLIP_BOUNDARY_FILE: "data/clip_boundary.geojson"
      CLIP_BOUNDARY_EPSG: "4326"            # set to your boundaryâ€™s CRS EPSG if not 4326
      ACTIVE_FIRES_CLIPPED: "active_fires.clipped.geojson"
      OUT_FIRES_CLIPPED: "out_fires.clipped.geojson"
      CWFIS_HOTSPOTS_24_CLIPPED: "hotspots_last24hrs.clipped.geojson"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Fetch ERD layers, validate, and write files
        shell: bash
        run: |
          set -euo pipefail
          python -u - << 'PYCODE'
          import os, time, json, hashlib
          import requests

          SESSION = requests.Session()
          SESSION.headers.update({"User-Agent": "github-actions ERD fetcher"})

          def log(msg):
              ts = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
              print(f"[{ts} UTC] {msg}", flush=True)

          def arcgis_get(url, params, *, attempts=3, timeout=60):
              for attempt in range(1, attempts + 1):
                  try:
                      r = SESSION.get(url, params=params, timeout=timeout)
                      if r.status_code == 200:
                          return r
                      log(f"HTTP {r.status_code} from {url} (attempt {attempt})")
                  except Exception as e:
                      log(f"Request error (attempt {attempt}): {e}")
                  time.sleep(1)
              raise RuntimeError(f"Failed after {attempts} attempts: {url}")

          def fetch_layer_meta(url):
              meta_url = url.rsplit("/query", 1)[0]
              r = arcgis_get(meta_url, {"f": "json"})
              meta = r.json()
              if not isinstance(meta, dict):
                  raise RuntimeError("Layer metadata not a dict")
              return meta

          def get_oid_field(meta):
              if isinstance(meta.get("objectIdField"), str):
                  return meta["objectIdField"]
              for f in meta.get("fields", []):
                  if f.get("type") == "esriFieldTypeOID":
                      return f.get("name")
              for cand in ("OBJECTID","ObjectID","objectid"):
                  if any(cand == f.get("name") for f in meta.get("fields", [])):
                      return cand
              return "OBJECTID"

          def get_all_field_names(meta):
              return [f["name"] for f in meta.get("fields", []) if "name" in f]

          def features_to_signature(fc):
              feats = fc.get("features", [])
              def feature_key(f):
                  props = f.get("properties", {}) or {}
                  for key in ("OBJECTID","ObjectID","objectid"):
                      if key in props:
                          return (0, props[key])
                  if "id" in f:
                      return (0, f["id"])
                  return (1, hashlib.sha256(json.dumps(props, sort_keys=True, ensure_ascii=False).encode("utf-8")).hexdigest())
              norm = []
              for f in feats:
                  props = f.get("properties", {}) or {}
                  strings = []
                  for _, v in sorted(props.items()):
                      if isinstance(v, str):
                          strings.append(v)
                  norm.append(("".join(strings), feature_key(f)))
              norm.sort(key=lambda t: t[1])
              concat = "|".join(s for s, _ in norm)
              return len(feats), hashlib.sha256(concat.encode("utf-8")).hexdigest()

          def table_to_signature(tbl):
              feats = tbl.get("features", [])
              rows = []
              for f in feats:
                  attrs = f.get("attributes", {}) or {}
                  strings = []
                  for _, v in sorted(attrs.items()):
                      if isinstance(v, str):
                          strings.append(v)
                  rows.append("".join(strings))
              rows.sort()
              concat = "|".join(rows)
              return len(feats), hashlib.sha256(concat.encode("utf-8")).hexdigest()

          def fetch_geojson(url):
              params = {
                  "where": "1=1",
                  "outFields": "*",
                  "outSR": "4326",
                  "returnGeometry": "true",
                  "f": "geojson",
              }
              r = arcgis_get(url, params)
              data = r.json()
              if isinstance(data, dict) and data.get("error"):
                  raise RuntimeError(f"ArcGIS error (geojson): {data['error']}")
              if data.get("type") != "FeatureCollection":
                  raise RuntimeError("GeoJSON did not contain a FeatureCollection")
              return data

          def fetch_attributes(url):
              params = {
                  "where": "1=1",
                  "outFields": "*",
                  "returnGeometry": "false",
                  "f": "json",
              }
              r = arcgis_get(url, params)
              data = r.json()
              if isinstance(data, dict) and data.get("error"):
                  raise RuntimeError(f"ArcGIS error (json attrs): {data['error']}")
              return data

          def attributes_index_by_oid(attrs_json, oid_field):
              index = {}
              for f in attrs_json.get("features", []):
                  attrs = f.get("attributes") or {}
                  if oid_field in attrs:
                      index[attrs[oid_field]] = attrs
              return index

          def merge_geojson_with_attrs(geojson_obj, attrs_by_oid, all_fields, oid_field):
              for f in geojson_obj.get("features", []):
                  props = f.get("properties")
                  if not isinstance(props, dict):
                      props = {}
                      f["properties"] = props
                  oid_val = (
                      props.get(oid_field)
                      or props.get(oid_field.upper())
                      or props.get(oid_field.lower())
                      or f.get("id")
                  )
                  source = attrs_by_oid.get(oid_val)
                  if not source:
                      continue
                  for name in all_fields:
                      if name not in props and name in source:
                          props[name] = source[name]
              return geojson_obj

          def add_timestamp_to_geojson(data, ts):
              for f in data.get("features", []):
                  props = f.get("properties")
                  if not isinstance(props, dict):
                      props = {}
                      f["properties"] = props
                  props["FETCHED_FROM_ERD"] = int(ts)
              return data

          def add_timestamp_to_table(data, ts):
              for f in data.get("features", []):
                  attrs = f.get("attributes")
                  if not isinstance(attrs, dict):
                      attrs = {}
                      f["attributes"] = attrs
                  attrs["FETCHED_FROM_ERD"] = int(ts)
              return data

          def attempt_fetch_geojson_with_full_attrs(kind, url, outfile, max_attempts=5, copies=4):
              meta = fetch_layer_meta(url)
              oid_field = get_oid_field(meta)
              all_fields = get_all_field_names(meta)
              log(f"{kind}: OID field='{oid_field}', fields={len(all_fields)}")

              for attempt in range(1, max_attempts + 1):
                  log(f"=== {kind}: attempt {attempt}/{max_attempts} ===")
                  payloads, sigs = [], []
                  for i in range(copies):
                      base_geo = fetch_geojson(url)
                      attrs_json_resp = fetch_attributes(url)
                      attrs_by_oid = attributes_index_by_oid(attrs_json_resp, oid_field)
                      merged = merge_geojson_with_attrs(base_geo, attrs_by_oid, all_fields, oid_field)
                      count, sig = features_to_signature(merged)
                      payloads.append(merged)
                      sigs.append((count, sig))
                      log(f"{kind}: copy {i+1}/{copies} -> features={count} sig={sig[:12]}")
                      time.sleep(1)
                  if all(sigs[0] == s for s in sigs[1:]):
                      ts = int(time.time())
                      final = add_timestamp_to_geojson(payloads[0], ts)
                      with open(outfile, "w", encoding="utf-8") as f:
                          json.dump(final, f, ensure_ascii=False, indent=2)
                      log(f"{kind}: wrote {outfile} (features={sigs[0][0]})")
                      return True
                  log(f"{kind}: copies mismatch on attempt {attempt}; retrying â€¦")
              log(f"{kind}: FAILED after {max_attempts} attempts")
              return False

          def attempt_fetch_table_json(kind, url, outfile, max_attempts=5, copies=4):
              for attempt in range(1, max_attempts + 1):
                  log(f"=== {kind}: attempt {attempt}/{max_attempts} ===")
                  payloads, sigs = [], []
                  for i in range(copies):
                      params = {"where":"1=1","outFields":"*","returnGeometry":"false","f":"json"}
                      r = arcgis_get(url, params)
                      data = r.json()
                      if isinstance(data, dict) and data.get("error"):
                          raise RuntimeError(f"ArcGIS error for {kind}: {data['error']}")
                      payloads.append(data)
                      count, sig = table_to_signature(data)
                      sigs.append((count, sig))
                      log(f"{kind}: copy {i+1}/{copies} -> rows={{count}} sig={sig[:12]}".format(count=count))
                      time.sleep(1)
                  if all(sigs[0] == s for s in sigs[1:]):
                      ts = int(time.time())
                      final = add_timestamp_to_table(payloads[0], ts)
                      with open(outfile, "w", encoding="utf-8") as f:
                          json.dump(final, f, ensure_ascii=False, indent=2)
                      log(f"{kind}: wrote {outfile} (rows={sigs[0][0]})")
                      return True
                  log(f"{kind}: copies mismatch on attempt {attempt}; retrying â€¦")
              log(f"{kind}: FAILED after {max_attempts} attempts")
              return False

          ok = True
          ok &= attempt_fetch_geojson_with_full_attrs("active_fires (layer 0)", os.environ["ACTIVE_FIRES_URL"], os.environ["ACTIVE_FIRES_FILE"])
          ok &= attempt_fetch_geojson_with_full_attrs("out_fires (layer 1)", os.environ["OUT_FIRES_URL"], os.environ["OUT_FIRES_FILE"])
          ok &= attempt_fetch_table_json("sums_table (layer 2)", os.environ["SUMS_TABLE_URL"], os.environ["SUMS_TABLE_FILE"])

          if not ok:
              log("One or more ERD datasets failed to stabilize; proceeding with whatever succeeded.")
          PYCODE

      - name: Fetch CWFIS hotspots (WFS, last 24 hours)
        shell: bash
        run: |
          set -euo pipefail
          python -u - << 'PY'
          import os, json, time, requests
          S = requests.Session()
          S.headers.update({"User-Agent":"github-actions cwfis fetcher"})
          params = {
            "service":"WFS","version":"1.0.0","request":"GetFeature",
            "typeName": os.environ["CWFIS_HOTSPOTS_TYPENAME_24H"],
            "maxFeatures":"100000","outputFormat":"application/json"
          }
          r = S.get(os.environ["CWFIS_WFS_ENDPOINT"], params=params, timeout=120)
          r.raise_for_status()
          data = r.json()
          ts = int(time.time())
          for f in data.get("features", []):
              props = f.get("properties") or {}
              f["properties"] = props
              props["FETCHED_FROM_CWFIS"] = ts
          with open(os.environ["CWFIS_HOTSPOTS_24_FILE"], "w", encoding="utf-8") as f:
              json.dump(data, f, ensure_ascii=False, indent=2)
          print(f"Wrote {os.environ['CWFIS_HOTSPOTS_24_FILE']} with {len(data.get('features',[]))} features.")
          PY

      # ---------- CLIP (robust, CRS-safe, with logging) ----------
      - name: Install Node (for mapshaper)
        if: env.DO_CLIP == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Clip to boundary (and clean)
        if: env.DO_CLIP == 'true'
        shell: bash
        run: |
          set -euo pipefail
          # Sanity checks
          test -f "$CLIP_BOUNDARY_FILE" || { echo "Clip boundary not found at $CLIP_BOUNDARY_FILE"; exit 1; }
          mkdir -p tmp

          echo "==> Reprojecting boundary to EPSG:4326 (from EPSG:$CLIP_BOUNDARY_EPSG) ..."
          npx --yes mapshaper -quiet \
            -i "$CLIP_BOUNDARY_FILE" crs=EPSG:$CLIP_BOUNDARY_EPSG \
            -proj EPSG:4326 \
            -clean \
            -o format=geojson tmp/boundary_4326.geojson

          echo "==> Counts BEFORE clip:"
          for f in "$ACTIVE_FIRES_FILE" "$OUT_FIRES_FILE" "$CWFIS_HOTSPOTS_24_FILE"; do
            test -f "$f" && echo -n "$f: " && jq '.features | length' "$f" || true
          done

          echo "==> Clipping (this will never change attributes, only geometries/features kept) ..."
          npx --yes mapshaper -quiet -i "$ACTIVE_FIRES_FILE" -clip tmp/boundary_4326.geojson -clean -o format=geojson "$ACTIVE_FIRES_CLIPPED"
          npx --yes mapshaper -quiet -i "$OUT_FIRES_FILE"    -clip tmp/boundary_4326.geojson -clean -o format=geojson "$OUT_FIRES_CLIPPED"
          npx --yes mapshaper -quiet -i "$CWFIS_HOTSPOTS_24_FILE" -clip tmp/boundary_4326.geojson -clean -o format=geojson "$CWFIS_HOTSPOTS_24_CLIPPED"

          echo "==> Counts AFTER clip:"
          for f in "$ACTIVE_FIRES_CLIPPED" "$OUT_FIRES_CLIPPED" "$CWFIS_HOTSPOTS_24_CLIPPED"; do
            test -f "$f" && echo -n "$f: " && jq '.features | length' "$f" || true
          done

      - name: Commit and push (if changed)
        shell: bash
        run: |
          set -euo pipefail
          git status --porcelain
          CHANGED="$(git status --porcelain)"
          if [ -n "$CHANGED" ]; then
            echo "Changes detected; committingâ€¦"
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"

            # Always add raw downloads
            git add "${ACTIVE_FIRES_FILE}" "${OUT_FIRES_FILE}" "${SUMS_TABLE_FILE}" "${CWFIS_HOTSPOTS_24_FILE}" || true

            # Optionally add clipped outputs if produced
            if [ "${DO_CLIP}" = "true" ]; then
              git add "${ACTIVE_FIRES_CLIPPED}" "${OUT_FIRES_CLIPPED}" "${CWFIS_HOTSPOTS_24_CLIPPED}" || true
            fi

            git commit -m "Update ERD & CWFIS fire datasets (+ robust clipping)" -m "Run: $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            git push
            echo "Pushed updates."
          else
            echo "No changes to commit."
          fi
